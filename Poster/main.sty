\documentclass[a0paper,landscape,margin=15mm,innermargin=12mm,colspace=10mm]{tikzposter}
\usepackage{lmodern}
\usepackage{amsmath,amssymb,mathtools}
% [demo] is used here so it compiles without the actual image file. 
% Remove [demo] when you use your real 'fig4.png'.
\usepackage[demo]{graphicx} 
\usepackage{caption}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, shapes, positioning}
\everymath{\displaystyle}


% Choose theme
\usetheme{Board}

% --- FIXED TITLE BLOCK ---
\makeatletter % <--- REQUIRED to use \@title, \@author, etc.
\settitle{\centering
  \color{titlefgcolor}
  % 1. LEFT PAPER (20% width)
  \begin{minipage}[c]{0.22\linewidth} % <--- This was previously commented out!
    \centering
    \normalsize\itshape
    A Connection Between Score Matching and Denoising Autoencoders\\[0.2em]
    \small\upshape P. Vincent (2011)
  \end{minipage}
  \hfill
  % 2. CENTER TITLE (50% width)
  \begin{minipage}[c]{0.52\linewidth}
    \centering
    \bfseries\huge\@title\par
    \vspace{0.4em}
    \large\@author\par
    \vspace{0.2em}
    \normalsize\@institute
  \end{minipage}%
  \hfill
  % 3. RIGHT PAPER (20% width)
  \begin{minipage}[c]{0.22\linewidth}
    \centering
    \normalsize\itshape
    Generative Modeling by Estimating Gradients of the Data Distribution\\[0.2em]
    \small\upshape Y. Song \& S. Ermon (2019)
  \end{minipage}
}
\makeatother % <--- Close the command access
% ----------------------------

\title{\textbf{From Denoising to Scores}\\Denoising Autoencoders to Noise-Conditional Score Networks}
\author{Ilan Aliouchouche, Cl\'ement Marie, Karim Rochd}
\institute{ENS Paris-Saclay \textbullet\ Master MVA \textbullet\ 2025}

\begin{document}

\maketitle[titletoblockverticalspace=10mm] 

\begin{columns}
% ==================== COLUMN 1 ====================
\column{0.32}
\block{\Large\bfseries Context \& Goal}{
  Generative modeling aims to sample from complex data  
  \textbf{without explicit likelihoods or normalizing constants}.  
  Classical maximum likelihood is often difficult because  
  \textit{partition functions are intractable}.  

  \vspace{0.6em}
  Score-based methods avoid this by learning  
  \textbf{the gradient of the log-density}:  
  the direction in which likelihood increases.  

  \vspace{0.6em}
  Vincent (2011) showed that denoising autoencoders  
  \textbf{implicitly learn the score of a smoothed density}.  

  \vspace{0.6em}
  Song \& Ermon (2019) extend this idea across  
  \textbf{multiple noise levels}, yielding  
  noise-conditional score networks that enable stable,  
  diffusion-like generative sampling.
}

\block{\Large\bfseries Denoising Autoencoders (DAE)}{

{
\centering
\begin{tikzpicture}[thick,scale=1.4, every node/.style={transform shape},
  arrowstyle/.style={-{Latex[length=5mm,width=4mm]}}
]

  \node[circle, draw, minimum size=1.8cm] (x)  at (-1,0) {$x$};
  \node[circle, draw, minimum size=1.8cm] (xt) at (4,1.2) {$\tilde{x}$};
  \node[circle, draw, minimum size=1.8cm] (h)  at (9,0) {$h$};
  \node[circle, draw, minimum size=1.8cm] (L)  at (4,-1.2) {$\mathcal{L}$};

  % FlÃ¨ches avec pointes visibles
  \draw[arrowstyle, ultra thick] (x) -- node[midway, above, xshift=-16pt] {\normalsize\textbf{$q(\tilde{x}\mid x)$}} (xt);
  \draw[arrowstyle, ultra thick] (xt) -- node[midway, above, xshift=18pt] {\normalsize\textbf{$f_{\theta}$}} (h);
  \draw[arrowstyle, ultra thick] (h) -- node[midway, right, xshift=-6pt, yshift=-12pt] {\normalsize\textbf{$g_{\phi}$}} (L);
  \draw[arrowstyle, ultra thick] (x) -- (L);

\end{tikzpicture}
\par
}

  \vspace{1em}

  A clean sample $x$ is corrupted into $\tilde{x}$ through  
  $q(\tilde{x}\mid x)$ (e.g. Gaussian noise).  
  The encoder $f_\theta$ processes $\tilde{x}$, the decoder $g_\phi$
  reconstructs $\hat{x}$, and the model learns to denoise:
    \[
      \mathcal L(\theta,\phi)
        = \mathbb{E}_{x,\tilde{x}}
          \bigl[\|g_\phi(f_\theta(\tilde{x})) - x\|^2\bigr].
    \]
}
%\block{\Large\bfseries From DAE to Score Matching}{
%  \centering
%  
%  % --- Section 1 ---
%  \textbf{\large 1. Why Scores? (Bypassing $Z$)} \\
%  Likelihood $p(x) = \frac{\tilde{p}(x)}{Z}$ is intractable because $Z$ is %unknown. \\
%  Taking the gradient makes $Z$ vanish:
%  \[
%     \nabla_x \log p(x) = \nabla_x \log \tilde{p}(x) - \underbrace{\nabla_x \log %Z}_{0}
%  \]
%
%
%  \vspace{0.8em}
%
%  % --- Section 2 ---
%  \textbf{\large 2. The Equivalence} \\
%  Vincent (2011) proved a surprising duality:
%  \[
%     \text{Minimizing Denoising Error} \iff \text{Score Matching}
%  \]
%  The DAE effectively learns the score of the smoothed density $q_\sigma(x)$.
%}

\block{\Large\bfseries From DAE to Score Matching}{

\begin{minipage}[t]{0.47\linewidth}
  \textbf{\large Why Scores?}\\[-1.5em]
  
  - Likelihood is intractable due to the constant $ Z = \int \tilde{p}(x)\,dx$.
  
  - Score : \\
  $\nabla_x \log p(x)
     = \nabla_x \log \tilde{p}(x)
     - \underbrace{\nabla_x \log Z}_{0}$

\end{minipage}
\hfill
\vrule width 1pt
\hfill
\begin{minipage}[t]{0.47\linewidth}

  \textbf{\large Equivalence}\\[-2.8em]

  \[
    \text{Minimizing Denoising Error}
  \]
  \[
    \iff
  \]
  \[
    \text{Score Matching}
  \]

  (Vincent, 2011).

\end{minipage}

\vspace{0.1em}

\centering
\includegraphics[width=0.75\linewidth]{scoresdae.png}

}

% ==================== COLUMN 2 ====================
\column{0.36}

% --- KEY INSIGHT BLOCK ---
\block{\Large\bfseries $\star$ Key Insight}{
  \large
  \textbf{Denoising Autoencoders} learn the local score of a smoothed data density:\\
  undoing Gaussian noise is equivalent to following the gradient of log-likelihood.

  \vspace{0.6em}
  \textbf{Noise-Conditional Score Networks} extend this to multiple noise scales,\\
  providing stable score estimates both near and far from the data manifold.

  \vspace{0.6em}
  \textbf{Annealed Langevin Dynamics} then uses these scores to generate samples,\\
  progressively shaping pure noise into the structure of real data.
}
% -------------------------
\block{\Large\bfseries Why Plain Score Matching Fails}{
  Real data lives on a \textbf{thin low-dimensional manifold}
  inside a high-dimensional space.  
  Near this manifold, the score is meaningful;
  far away, density is almost zero and the score becomes \textbf{ill-conditioned}.

  \vspace{0.6em}
  As a result, \textbf{naive single-scale score matching} produces
  noisy, unreliable gradients off the manifold,  
  causing Langevin sampling to \textbf{mix poorly or even diverge}.

  \vspace{0.6em}
  These geometric limitations make a single-scale score model insufficient,  
  motivating \textbf{multiscale noise levels} and the NCSN framework.
}

\block{\Large\bfseries Noise-Conditional Score Networks}{
  A single network \(s_\theta(x,\sigma)\) learns the score of the 
  \textbf{data + Gaussian noise} at multiple noise scales 
  \(\sigma_1 > \cdots > \sigma_L\).

  \vspace{0.6em}
  Training uses \textbf{denoising score matching at each level}:
  \[
    \mathcal{L}(\theta)=\frac{1}{L}\sum_{i=1}^L \sigma_i^{2} 
    \mathbb{E}\Bigl[
      \| s_\theta(\tilde{x},\sigma_i)
        + \tfrac{\tilde{x}-x}{\sigma_i^2}\|^2
    \Bigr].
  \]

  \vspace{0.6em}
  \textbf{Large \(\sigma\):} captures global, coarse structure.\\
  \textbf{Small \(\sigma\):} captures fine, detailed variations.

  \vspace{0.4em}
  This multiscale view \emph{stabilizes score learning} far from the data manifold.
}

% ==================== COLUMN 3 ====================

% ==================== COLUMN 3 (Karim) ====================
\column{0.32}

% --- UNIFYING VIEW BLOCK ---
\block{\Large\bfseries The Bridge: From DAE to NCSN}{
  \textbf{Vincent (2011)} proved that a Denoising Autoencoder (DAE) with noise $\sigma$ learns the score of the smoothed density $q_\sigma(x)$:
  \[
      \text{Optimal DAE Residual} \approx \sigma^2 \nabla_x \log q_\sigma(x)
  \]
  
  \vspace{0.5em}
  \textbf{The Problem:} A single $\sigma$ is insufficient.
  \begin{itemize}
      \item Small $\sigma$: Score is undefined off the manifold (blind to global structure).
      \item Large $\sigma$: Valid score, but data is blurry.
  \end{itemize}

  \vspace{0.5em}
  \textbf{The Solution (Song \& Ermon, 2019):} 
  Train a single network $s_\theta(x, \sigma)$ conditioned on \textbf{all} noise levels.
  
  \begin{center}
      \textbf{High $\sigma \xrightarrow{Annealing} Low \sigma$}\\
      \textit{(Global Navigation)} $\to$ \textit{(Fine Details)}
  \end{center}
}

% --- RESULTS BLOCK ---
% --- RESULTS BLOCK ---
\block{\Large\bfseries Results}{
  NCSN achieves state-of-the-art \textbf{Inception Score} on CIFAR-10 without adversarial training, outperforming spectral normalized GANs (SNGAN).

  \vspace{0.5em}
  \centering
  \begin{tikzfigure}[Inception and FID Scores on CIFAR-10 (Unconditional)]
  \begin{tabular}{l c c}
      \toprule
      \textbf{Model} & \textbf{Inception} $\uparrow$ & \textbf{FID} $\downarrow$ \\
      \midrule
      PixelCNN & 4.60 & 65.93 \\
      WGAN-GP & 7.86 & 36.40 \\
      SNGAN & 8.22 & 21.70 \\
      \textbf{NCSN (Ours)} & \textbf{8.87} & \textbf{25.32} \\
      \bottomrule
  \end{tabular}
  \end{tikzfigure}
  
  \vspace{1em}
  
  % Images placed side-by-side
  % Note: Ensure the widths sum to < 1.0 (e.g., 0.45 + 0.45 = 0.9)
  \includegraphics[width=0.49\linewidth]{fig41.jpeg}
  \hfill % This adds flexible space between the images
  \includegraphics[width=0.49\linewidth]{fig42.jpeg}
  
  \vspace{0.2em}
  {\small \textit{Generated samples from CIFAR-10.}}
}

% --- TAKEAWAYS / LIMITATIONS BLOCK ---
\block{\Large\bfseries Takeaways \& Limitations}{
  \begin{minipage}[t]{0.55\linewidth}
    \textbf{\textcolor{titlefgcolor}{Key Takeaways}}
    \begin{itemize}
        \item \textbf{Denoising = Scores:} We can learn gradients without calculating partition functions.
        \item \textbf{Conditioning:} One network can learn the geometry of data at multiple scales.
        \item \textbf{Annealing:} Sampling must proceed from coarse to fine to mix between modes.
    \end{itemize}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.40\linewidth}
    \textbf{\textcolor{red}{Limitations}}
    \begin{itemize}
        \item \textbf{Slow Sampling:} Requires thousands of forward passes ($T \times L$).
        \item \textbf{Bias:} $\sigma > 0$ implies we sample from a slightly smoothed distribution.
    \end{itemize}
  \end{minipage}
}

\end{columns}
\end{document}